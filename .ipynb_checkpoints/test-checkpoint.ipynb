{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os.path\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "######### Function for reading / writing data #########\n",
    "\n",
    "def read_parameters(path):\n",
    "    '''\n",
    "    Returns parameters written in \n",
    "    parameter.txt file.\n",
    "\n",
    "    Input: path to parameter.txt file\n",
    "    outputted by initiate_hmm.py\n",
    "    '''\n",
    "    parameter_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            if len(line)==0:\n",
    "                continue\n",
    "            x = re.search('(?<=\\=).*',line[0]).group()\n",
    "            dtype = re.match('\\([a-zA-Z]+\\)',line[0]).group()\n",
    "            dtype = dtype[1:-1]\n",
    "            if dtype == 'int':\n",
    "                parameter_list.append(int(x.strip()))\n",
    "            elif dtype == 'float':\n",
    "                 parameter_list.append(float(x.strip()))\n",
    "            elif dtype == 'string':\n",
    "                 parameter_list.append(x.strip())\n",
    "    return parameter_list\n",
    "\n",
    "def make_unique_file_or_dir_names(path):\n",
    "    '''\n",
    "    Returns new file/directory name.\n",
    "\n",
    "    Using the path parameter, checks if the \n",
    "    given file/directory exists in the path,\n",
    "    and creates a new name for the file/directory.\n",
    "\n",
    "    *designed so that user puts in same file \n",
    "    name multiple times and this function creates\n",
    "    filename with increasing index number at the end.\n",
    "    ex)\n",
    "        path = dir/file_name.csv\n",
    "        run this funciton three times:\n",
    "        dir/file_name_1.csv\n",
    "        dir/file_name_2.csv\n",
    "        dir/file_name_3.csv\n",
    "        \n",
    "    '''\n",
    "    filename, extension = os.path.splitext(path)\n",
    "    \n",
    "    first_path = filename+'_1'+extension\n",
    "    if os.path.exists(first_path) == False:\n",
    "        return first_path\n",
    "    else:\n",
    "        counter = 2\n",
    "        while os.path.exists(first_path):\n",
    "            first_path = filename + \"_\" + str(counter) + extension\n",
    "            counter += 1\n",
    "        return first_path\n",
    "\n",
    "def df_to_csv_for_DMM(otu_df, filepath):\n",
    "    '''\n",
    "    Saves the DF as csv file @\n",
    "    \"MicrobeDMMv1.0/Data/\"+filename\n",
    "    Returns the filepath to the outputted csv file.\n",
    "    If csv file existed before at \"MicrobeDMMv1.0/Data,\" \n",
    "    filepath with new file name will be returned. \n",
    "\n",
    "    Input: \n",
    "    1. OTU DF\n",
    "    2. file path\n",
    "\n",
    "    If otu_table.csv already exists, \n",
    "    this function saves the file in\n",
    "    a different name.\n",
    "    '''\n",
    "    filename = re.search('[a-zA-Z_]+.csv',filepath).group()\n",
    "    path = make_unique_file_or_dir_names(\"MicrobeDMMv1.0/Data/\"+filename)\n",
    "\n",
    "    column_names = list(otu_df)\n",
    "    taxa = otu_df[\"Taxa\"]\n",
    "    otu_df.drop(columns=[\"Taxa\"],inplace=True)\n",
    "    col_names = [i for i in range(len(otu_df.columns))]\n",
    "\n",
    "    otu_df.columns = col_names\n",
    "    otu_df.insert(0,\"Taxa\",taxa)\n",
    "    otu_df.to_csv(path, index=False)\n",
    "    otu_df.columns = column_names\n",
    "    print(\"File saved as:\"+ path)\n",
    "    print()\n",
    "    return path\n",
    "\n",
    "######### Functions for cleaning and priming data for DMM and HMM #########\n",
    "\n",
    "def read_data_as_df(file_name):\n",
    "    '''\n",
    "    Returns OTU DF and a Series \n",
    "    of bacteria taxa name.\n",
    "    \n",
    "    Input: OTU csv file\n",
    "    \n",
    "    Renames column of taxa name to \"Taxa\"\n",
    "    \n",
    "    '''\n",
    "    otu_df = pd.read_csv(file_name, engine='python')\n",
    "    if 'ID' in list(otu_df):\n",
    "        otu_df.rename(columns={\"ID\":\"Taxa\"},inplace=True)\n",
    "    return otu_df\n",
    "\n",
    "def convert_data_to_int(otu_df):\n",
    "    '''\n",
    "    Returns OTU DF with taxa abundance \n",
    "    values as int type.\n",
    "    \n",
    "    Input: OTU DF\n",
    "    \n",
    "    Sorts sample ID (column names) lexicographically.\n",
    "    Multiplies data by a 1e6 & adds 1 \n",
    "    to all data to rid of zeros.\n",
    "    \n",
    "    '''\n",
    "    if \"Taxa\" in list(otu_df):\n",
    "        taxa = otu_df[\"Taxa\"]\n",
    "        numeric_df = otu_df.drop(columns=['Taxa'])\n",
    "    else:\n",
    "        numeric_df = list(otu_df)\n",
    "    \n",
    "    numeric_df.sort_index(axis=1,inplace=True)\n",
    "    numeric_df = numeric_df.multiply(1e6).round().astype(int)\n",
    "    numeric_df = numeric_df.add(1)\n",
    "    numeric_df.insert(0,\"Taxa\",taxa)\n",
    "    \n",
    "    return numeric_df\n",
    "\n",
    "def calc_variance(otu_df):\n",
    "    '''\n",
    "    Takes bacteria taxa by samples abundance DF as a parameter.\n",
    "    Calculate variance of each row.\n",
    "    returns a list of tuples (row idx, variances).\n",
    "    '''\n",
    "    if \"Taxa\" in list(otu_df):\n",
    "        numeric_df = otu_df.drop(columns=\"Taxa\")\n",
    "    else:\n",
    "        numeric_df = otu_df\n",
    "        \n",
    "    n = len(numeric_df.columns)\n",
    "    mean = np.array([])\n",
    "    for row in numeric_df.index:\n",
    "        mean = np.append(mean, np.mean(numeric_df.loc[row,:]))\n",
    "    numeric_df[\"mean\"] = mean\n",
    "    \n",
    "    var = []\n",
    "    for idx,row in numeric_df.iterrows():\n",
    "        deviations_sum = ((numeric_df.loc[idx,:] - mean[idx])**2).sum()\n",
    "        var.append((idx,(deviations_sum / n)))\n",
    "    var = sorted(var,key=lambda x:x[1])\n",
    "    \n",
    "    return var\n",
    "\n",
    "def choose_top_x_percent_of_taxa_with_highest_variance(otu_df, frac=0.0338):\n",
    "    '''\n",
    "    Returns otu_df with selected taxa (row).\n",
    "    Taxa is sorted from high to low variation \n",
    "    in data and taxa with only high variance \n",
    "    can be selected using frac parameter.\n",
    "    \n",
    "    Input: \n",
    "    1.otu_df\n",
    "    2. fraction of taxa to be dropped\n",
    "    \n",
    "    '''\n",
    "    print('Number of taxa (rows) before selection:', len(otu_df))\n",
    "    \n",
    "    variance = calc_variance(otu_df)\n",
    "    \n",
    "    new_taxa_len = int(len(otu_df)*frac)\n",
    "    len_of_taxa_to_be_dropped = len(otu_df) - new_taxa_len\n",
    "    \n",
    "    indices_to_be_dropped=[]\n",
    "    for var in variance[:len_of_taxa_to_be_dropped]:\n",
    "        indices_to_be_dropped.append(var[0])\n",
    "    \n",
    "    otu_df_dropped_rows = otu_df.drop(index=indices_to_be_dropped)\n",
    "    \n",
    "    taxa_name = otu_df_dropped_rows[\"Taxa\"]\n",
    "    \n",
    "    print('Number of taxa (rows) after selection:', len(otu_df_dropped_rows))\n",
    "    print()\n",
    "    \n",
    "    return otu_df_dropped_rows, taxa_name\n",
    "\n",
    "def drop_infants_with_less_than_5_samples(otu_df):\n",
    "    '''\n",
    "    Returns OTU DF after dropping \n",
    "    infants with less than 5 samples.\n",
    "    \n",
    "    Input: OTU DF\n",
    "    '''\n",
    "    \n",
    "    column_names = list(otu_df)[1:]\n",
    "\n",
    "    # extract tuple of (id + site, column name) as list of sets\n",
    "    infants = []\n",
    "    for col_name in column_names:\n",
    "        #search for id+site (1001_A)\n",
    "        infants.append((re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]\",col_name).group(), col_name)) \n",
    "    infants = sorted(infants, key = lambda x: x[0] )\n",
    "    \n",
    "    #drop ID with less than 5 samples\n",
    "    cnt = 0\n",
    "    tmp_id = []\n",
    "    for i,infant in enumerate(infants):    \n",
    "        cnt = cnt+1\n",
    "        tmp_id.append(infant[1])\n",
    "        curr_id = infant[0]\n",
    "        if i != 0:\n",
    "            prev_id = infants[i-1][0]\n",
    "            if prev_id != curr_id:\n",
    "                last_element = tmp_id[len(tmp_id)-1]\n",
    "                if cnt <= 4: \n",
    "                    otu_df.drop(columns = tmp_id[:len(tmp_id)-1], inplace = True)\n",
    "                tmp_id.clear()\n",
    "                tmp_id.append(last_element)\n",
    "                cnt = 0\n",
    "    return otu_df\n",
    "\n",
    "######### Functions for data (nested list of infants) selection #########\n",
    "\n",
    "def convert_otu_df_to_data(otu_df, include_nan=True):\n",
    "    '''\n",
    "    Returns list of lists (= list of infants). \n",
    "    Each infant contrains one DF: abundance OTU DF\n",
    "\n",
    "    If include_nan is True, columns containing nan values will be included.\n",
    "\n",
    "    Input: OTU DF (int)\n",
    "    \n",
    "    Column name example: 1000_A_1 (ID_Site_timepoint)\n",
    "    SampleNum: positive integer\n",
    "    Site: single letter\n",
    "    '''\n",
    "    if \"Taxa\" in list(otu_df):\n",
    "        samples = list(otu_df.drop(columns=['Taxa']))\n",
    "    else:\n",
    "        samples = list(otu_df)\n",
    "\n",
    "    infants = []\n",
    "    \n",
    "    # find unique ID (sampleID_Site_)\n",
    "    for sample in samples:\n",
    "        infants.append(re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]_\",sample).group())\n",
    "    \n",
    "    unique_infants = sorted(list(set(infants)))\n",
    "    \n",
    "    # create empty dfs:\n",
    "    if include_nan == False:\n",
    "        tmp_data = []\n",
    "        for _ in unique_infants:\n",
    "            tmp_data.append([pd.DataFrame()])  \n",
    "    else:\n",
    "        tmp_data = []\n",
    "        for infant in unique_infants:\n",
    "            tmp_data.append([ pd.DataFrame(columns = [infant+str(x+1) for x in range(10)]) ]) \n",
    "\n",
    "    # Store abundance data.\n",
    "    index = 0\n",
    "    for i,sample in enumerate(samples):\n",
    "        identifier = re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]_\",sample).group()\n",
    "        if identifier == unique_infants[index]:\n",
    "            tmp_data[index][0][sample] = otu_df[sample] #add columns\n",
    "        elif i==len(samples)-1:\n",
    "            continue\n",
    "        else: \n",
    "            tmp_data[index+1][0][sample] = otu_df[sample] #add columns\n",
    "            index+=1\n",
    "    return tmp_data\n",
    "\n",
    "def convert_list_of_infants_to_otu_df(data, taxa_names):\n",
    "    '''\n",
    "    When converting list of infants to otu_df,\n",
    "    excludes nan value.\n",
    "    '''\n",
    "    container = []\n",
    "    for infant in data:\n",
    "        container.append(infant[0])\n",
    "    otu_df = pd.concat([pd.Series(taxa_names,name=\"Taxa\")])\n",
    "    container.insert(0,otu_df)\n",
    "    otu_df = pd.concat(container, axis=1)\n",
    "    \n",
    "    return otu_df\n",
    "\n",
    "######### DMM functions #########\n",
    "\n",
    "def run_dmm(csv_file_path, out, n_clusters = 6):\n",
    "    '''\n",
    "    Runs DMM in MicrobeDMMv1.0 directory.\n",
    "    DMM c code saves output as stub.z \n",
    "    in dmm_out folder.\n",
    "    \n",
    "    Input: \n",
    "    1. OTU Table csv file path\n",
    "    2. n_clusters (or n_states or hidden_label): \n",
    "    number of clusters. DMM  will \n",
    "    assign a cluster label to each sample.\n",
    "    \n",
    "    *n_clusters (n_states) value begins from 1, not 0.\n",
    "    *DMM code returns label of states that begins from 1.\n",
    "    '''\n",
    "    if os.path.exists('MicrobeDMMv1.0/DirichletMixtureGHPFit') == False:\n",
    "        subprocess.run(['make'], shell=True, cwd=\"MicrobeDMMv1.0\")\n",
    "\n",
    "    process = subprocess.Popen(['MicrobeDMMv1.0/DirichletMixtureGHPFit', \n",
    "                    '-in', csv_file_path, \n",
    "                    '-out', out, \n",
    "                    '-k', str(n_clusters),\n",
    "                    '-l', str(random.randint(1,7000)) ])\n",
    "    if process.wait() == 0:\n",
    "        print(\"Successfully ran DMM.\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Error occured while running DMM\")\n",
    "        print()\n",
    "\n",
    "def read_dmm_output(dmm_output_path, otu_df):\n",
    "    '''\n",
    "    Returns cluster DF with 2 columns: sample & cluster label.\n",
    "    Retrieves DMM output from stub.z file \n",
    "    @ MicrobeDMMv1.0/Data/otu_table.csv\n",
    "    \n",
    "    Inputs: \n",
    "    1. file path to stub.z \n",
    "    (DMM cluster assignment output)\n",
    "    2. OTU DF\n",
    "    \n",
    "    '''\n",
    "    dmm_out = []\n",
    "    with open(dmm_output_path,\"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        _ = next(reader) #remove header\n",
    "        for line in reader:\n",
    "            tmp=[]\n",
    "            for i, value in enumerate(line):\n",
    "                if i!=0:\n",
    "                    tmp.append(float(value))\n",
    "                else:\n",
    "                    tmp.append(value)\n",
    "            dmm_out.append(tmp)\n",
    "    \n",
    "    #select the cluster label with the highest probability\n",
    "    cluster_assignment=[]\n",
    "\n",
    "    for row in dmm_out:\n",
    "        max = -1.0\n",
    "        idx = -1\n",
    "        for j,v2 in enumerate(row):\n",
    "            if j!=0 and v2 > max:\n",
    "                max = v2\n",
    "                idx = j\n",
    "        cluster_assignment.append((row[0],idx))\n",
    "        \n",
    "    #output as pd.dataframe\n",
    "    cluster_df=pd.DataFrame(cluster_assignment)\n",
    "    cluster_df.columns=[\"sample\",\"cluster\"]\n",
    "\n",
    "    #rename 'sample' column values as the sample (column) names from OTU DF\n",
    "    cluster_df['sample'] = list(otu_df)[1:]\n",
    "\n",
    "    return cluster_df\n",
    "\n",
    "def check_if_all_states_are_included_in_dmm_output(cluster_assignments_df, n_clusters = 6):\n",
    "    '''\n",
    "    Returns 0 if the number if number of n_cluster \n",
    "    designated in run_dmm() by the user matches the \n",
    "    output of DMM code.\n",
    "    \n",
    "    If the numbers match, returns 1.\n",
    "    \n",
    "    Input: \n",
    "    1. Output from read_dmm_output\n",
    "    2. Number of states\n",
    "    \n",
    "    '''\n",
    "    rerun_dmm = False\n",
    "    \n",
    "    cluster_sr = cluster_assignments_df['cluster'].value_counts()\n",
    "    if len(cluster_sr) != n_clusters:\n",
    "        rerun_dmm = True\n",
    "    if rerun_dmm == True:\n",
    "        print('Number of DMM output states did not match the user input.')\n",
    "        print('Rerunning DMM')\n",
    "        print()\n",
    "        return 0 # sth went wrong\n",
    "    else:\n",
    "        print('Number of DMM output states matches the user input.')\n",
    "        print()\n",
    "        return 1 # correct\n",
    "\n",
    "def dmm(otu_df, csv_file_path, dmm_output_name, path_to_dmm_output_filename, n_clusters = 6,):\n",
    "    '''\n",
    "    Returns cluster assignments of all \n",
    "    the samples (columns of otu_df) as df.\n",
    "    Column 1: sample names\n",
    "    Column 2: cluster assignments\n",
    "\n",
    "    *This fuction is made to checks if the output df \n",
    "    contains the same number of unique clusters\n",
    "    users designated. Often, DMM output doesn't\n",
    "    contain all clusters(states).\n",
    "    '''\n",
    "\n",
    "    final_ok = False\n",
    "    while final_ok == False: \n",
    "        run_dmm(csv_file_path=csv_file_path, out= dmm_output_name, n_clusters=n_clusters)\n",
    "        cluster_assignments_df = read_dmm_output(path_to_dmm_output_filename, otu_df)\n",
    "\n",
    "        ################# outputs pie chart of dmm output ##################\n",
    "        cluster_assignments_df[\"cluster\"].value_counts().plot(kind=\"pie\",figsize=(6,6))\n",
    "        if os.path.exists('dmm_img_outputs') == False:\n",
    "            os.mkdir('dmm_img_outputs')\n",
    "\n",
    "        path = 'dmm_img_outputs/dmm_clusters.png'\n",
    "        new_path = make_unique_file_or_dir_names(path)\n",
    "        plt.savefig(new_path)\n",
    "        #####################################################################\n",
    "\n",
    "        final_ok = check_if_all_states_are_included_in_dmm_output(cluster_assignments_df, n_clusters=n_clusters)\n",
    "\n",
    "    return cluster_assignments_df\n",
    "\n",
    "def check_column_numbers_for_DMM(otu_df):\n",
    "    '''\n",
    "    Returns 1 if column numbers of otu_df \n",
    "    is smaller or equal than 1800.\n",
    "    Else, returns 0.\n",
    "    '''\n",
    "    if len(list(otu_df)) <= 1800:\n",
    "        return 1 #True\n",
    "    else:\n",
    "        return 0 #False\n",
    "\n",
    "def transpose_cluster_assignment_df(cluster_assignments_df):\n",
    "    columns_sr = list(cluster_assignments_df['sample'])\n",
    "    states_sr = cluster_assignments_df['cluster']\n",
    "    transposed_cluster_df = pd.DataFrame(columns=columns_sr,index=[0])\n",
    "    transposed_cluster_df.iloc[0]=states_sr\n",
    "    return transposed_cluster_df\n",
    "\n",
    "def transpose_cluster_df_back(c_df):\n",
    "    sample_names = list(c_df)\n",
    "    states_sr = list(c_df.iloc[0])\n",
    "    transposed_cluster_df = pd.DataFrame({'sample':sample_names,'cluster':states_sr})\n",
    "    return transposed_cluster_df\n",
    "\n",
    "def check_column_numbers_and_run_DMM(otu_df, csv_data_filepath, n_states, pma_start_date):\n",
    "    '''\n",
    "    Goal is the keep as many columns as possible before running DMM \n",
    "    so that more information is given to DMM for clustering.\n",
    "    \n",
    "    DMM code can be run using data matrix with less than 1800 columns.\n",
    "    If (n columns <= 1800), DMM is ran and then trimmed into the input format of HMM.\n",
    "    Else (n columns > 1800), data matrix is trimmed into the input format of HMM and then DMM is ran.\n",
    "    \n",
    "    dmm_output_as: ...dmm_out/stub\n",
    "        *used as a paramter for DMM code for dmm output name\n",
    "    dmm_output_path: ...dmm_out/stub.z \n",
    "        *used to read the output to get cluster assignment\n",
    "    '''\n",
    "    # make dmm_outputs directory. \n",
    "    # Everytime dmm is run, dmm_out_# directories will be saved here\n",
    "    if os.path.exists('dmm_outputs') == False:\n",
    "        os.mkdir('dmm_outputs')\n",
    "\n",
    "    path_to_dmm_out_dir = make_unique_file_or_dir_names('dmm_outputs/dmm_out')\n",
    "    os.mkdir(path_to_dmm_out_dir)\n",
    "\n",
    "    # if column # <= 1800 run dmm here\n",
    "    if check_column_numbers_for_DMM(otu_df) == True:\n",
    "        print('otu_table columns were not trimmed to run DMM')\n",
    "        print()\n",
    "        # gf_or_gn = re.search('/[a-zA-Z]+', data_filepath).group()\n",
    "\n",
    "        # to run DMM, must save modified otu table df in 'MicrobeDMMv1.0/Data'\n",
    "        path_to_dmm_otu_csv = df_to_csv_for_DMM(otu_df, csv_data_filepath) \n",
    "\n",
    "        # Run DMM before trimming columns\n",
    "        cluster_assignments_df = dmm(otu_df = otu_df,\\\n",
    "                                csv_file_path = path_to_dmm_otu_csv,\\\n",
    "                                dmm_output_name = path_to_dmm_out_dir+'/stub',\\\n",
    "                                path_to_dmm_output_filename = path_to_dmm_out_dir+'/stub.z',\\\n",
    "                                n_clusters = n_states)\n",
    "        \n",
    "        # trim columns of cluster df (because dmm was ran with all samples)\n",
    "        transposed_cluster_df = transpose_cluster_assignment_df(cluster_assignments_df)\n",
    "        trimmed_cluster_df = drop_infants_with_less_than_5_samples(transposed_cluster_df)\n",
    "        timepoints = select_10_pma(pma_start_date = pma_start_date) \n",
    "        selected_columns = sample_based_on_timepoints(trimmed_cluster_df, timepoints)\n",
    "        trimmed_cluster_df = drop_samples_not_part_of_timepoints(selected_columns, trimmed_cluster_df)\n",
    "        cluster_assignments_df = transpose_cluster_df_back(trimmed_cluster_df)\n",
    "\n",
    "        # trim columns (samples)\n",
    "        otu_df = drop_infants_with_less_than_5_samples(otu_df)\n",
    "        timepoints = select_10_pma(pma_start_date=pma_start_date) \n",
    "        selected_columns = sample_based_on_timepoints(otu_df, timepoints)\n",
    "        otu_df = drop_samples_not_part_of_timepoints(selected_columns, otu_df)\n",
    "        \n",
    "        return cluster_assignments_df, otu_df\n",
    "    else:\n",
    "        print('There were too many columns in the otu_table. Columns will be trimmed to run DMM')\n",
    "        print()\n",
    "        # trim columns (samples) 1\n",
    "        otu_df = drop_infants_with_less_than_5_samples(otu_df)\n",
    "        timepoints = select_10_pma(pma_start_date = pma_start_date) \n",
    "        selected_columns = sample_based_on_timepoints(otu_df, timepoints)\n",
    "        \n",
    "        # trim columns (samples) 2\n",
    "        otu_df = drop_samples_not_part_of_timepoints(selected_columns, otu_df)\n",
    "\n",
    "        #to run DMM, must save modified otu table df in 'MicrobeDMMv1.0/Data'\n",
    "        path_to_dmm_otu_csv = df_to_csv_for_DMM(otu_df, csv_data_filepath)\n",
    "\n",
    "        # no need to trim columns because DMM was ran after trimming\n",
    "        cluster_assignments_df = dmm(otu_df = otu_df,\\\n",
    "                                    csv_file_path = path_to_dmm_otu_csv,\\\n",
    "                                    dmm_output_name = path_to_dmm_out_dir+'/stub',\\\n",
    "                                    path_to_dmm_output_filename = path_to_dmm_out_dir+'/stub.z',\\\n",
    "                                    n_clusters = n_states)\n",
    "                                    \n",
    "        return cluster_assignments_df, otu_df\n",
    "\n",
    "######### Functions handling data (list of infants (infant = list of abundance df and state label df) #########\n",
    "\n",
    "def select_10_pma(pma_start_date=196):\n",
    "    '''\n",
    "    Returns a list of 10 PMA dates that are 7 days \n",
    "    apart from the start date, hence the addition of 63 from start date.\n",
    "    \n",
    "    Takes in start PMA date (int) as a parameter.\n",
    "    \n",
    "    *Estimate range of PMA dates for all infants: 0 - 273.\n",
    "    \n",
    "    Default input is 196 because most data were \n",
    "    able to be selected starting from 196 PMA.\n",
    "\n",
    "    Default output range: 196 - 260\n",
    "    '''\n",
    "    end = pma_start_date + 63\n",
    "    return list(range(pma_start_date, end+1, 7))\n",
    "\n",
    "def compare_pma_and_select_data(pma_and_id, timepoints):\n",
    "    '''\n",
    "    Returns list of samples that are selected\n",
    "    and those that are not selected. \n",
    "    All samples belong to a single infant.\n",
    "    \n",
    "    Input: \n",
    "    1. [PMA, sample ID] of all samples that belongs to one infant in a list\n",
    "    2. list of 10 PMA time points (output from select_10_pma())\n",
    "    \n",
    "    This function selects samples that are less than\n",
    "    3 days away from each time point. \n",
    "    \n",
    "    Selected samples contain list of 3 elements: \n",
    "    [PMA, sample ID, ID_Site_timepoint]\n",
    "    \n",
    "    Samples that weren't selected contain 2 elements.\n",
    "    '''\n",
    "    idx = 0 \n",
    "    for j,pma in enumerate(pma_and_id):\n",
    "        if timepoints[idx]-pma[0] < -3: \n",
    "            while idx < 10 and timepoints[idx]-pma[0] < -3:\n",
    "                idx+=1\n",
    "            if idx == 10:\n",
    "                break\n",
    "        if timepoints[idx]-pma[0] > 3:\n",
    "            continue  \n",
    "        elif 3 >= abs(timepoints[idx]-pma[0]):\n",
    "            if j > 0:\n",
    "                prev_d = timepoints[idx]-pma_and_id[j-1][0]\n",
    "                curr_d = timepoints[idx]-pma[0]\n",
    "                if 3 >= abs(prev_d) >= abs(curr_d):\n",
    "                    pma_and_id[j-1].pop()\n",
    "                if abs(curr_d) > abs(prev_d):\n",
    "                    continue\n",
    "                pma.append(re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]_\",pma[1]).group() + str(idx+1))\n",
    "            else:\n",
    "                pma.append(re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]_\",pma[1]).group() + str(idx+1))\n",
    "                \n",
    "    tmp = [i for i in pma_and_id]\n",
    "    pma_and_id.clear()\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "def sample_based_on_timepoints(df, timepoints):\n",
    "    '''\n",
    "    Returns a list of infants. Samples with \n",
    "    PMA date that falls within 3 days of \n",
    "    selected timepoints (output from select_10_pma()) \n",
    "    have len of 3. Samples that are not selected \n",
    "    have len of 2.\n",
    "    \n",
    "    Input: \n",
    "    1.OTU df\n",
    "    2.list of 10 time points\n",
    "    \n",
    "    Selected samples have lenth of 3:\n",
    "    [227, '1002_B_227', '1002_B_227_5'] \n",
    "    (pma, id_site_pma, id_site_pma_timepoint_label)\n",
    "    \n",
    "    Samples that's not selected have lenth of 2:\n",
    "    [227, '1002_B_227']\n",
    "    '''\n",
    "    pma_and_id = []\n",
    "    selected_columns = []\n",
    "    \n",
    "    if \"Taxa\" in list(df):\n",
    "        columns = list(df.drop(columns=['Taxa']))\n",
    "    else:\n",
    "        columns = list(df)\n",
    "    \n",
    "    for i,col in enumerate(columns):\n",
    "        curr_id = re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]\",col).group()\n",
    "        pma_and_id.append([int(re.search(\"[0-9]+$\",col).group()),col])\n",
    "        \n",
    "        if i != 0 & i !=len(columns)-1:\n",
    "            prev_id = re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]\",columns[i-1]).group()\n",
    "            if prev_id != curr_id:\n",
    "                last_element = pma_and_id[len(pma_and_id)-1]\n",
    "                pma_and_id.pop()\n",
    "                out = compare_pma_and_select_data(pma_and_id, timepoints)\n",
    "                selected_columns += out\n",
    "                pma_and_id.append(last_element)\n",
    "                \n",
    "        if i==len(columns)-1:\n",
    "            out = compare_pma_and_select_data(pma_and_id, timepoints)\n",
    "            selected_columns += out\n",
    "            \n",
    "    return selected_columns\n",
    "\n",
    "def drop_samples_not_part_of_timepoints(selected_columns, df):\n",
    "    '''\n",
    "    Returns OTU DF after dropping columns \n",
    "    not in selected_columns list\n",
    "    \n",
    "    Input: \n",
    "    1. list of selected samples (columns)\n",
    "    2. OTU DF \n",
    "    \n",
    "    Drops columns(samples) that are selected.\n",
    "    Replace PMA in all column names with timepoints (1-10).\n",
    "    '''\n",
    "    sth_went_wrong = False\n",
    "    for col in selected_columns:\n",
    "        if len(col) == 2:\n",
    "            df.drop(columns=col[1],inplace=True)\n",
    "        elif len(col) == 3:\n",
    "            df.rename(columns={col[1]:col[2]},inplace=True)\n",
    "        else:\n",
    "            sth_went_wrong == True\n",
    "    if sth_went_wrong == True:\n",
    "        print(\"Wrong_format: please check if all column names are formatted as so: \\\n",
    "            \\\"id(integer)\\\" + \\\"site(A-Z)\\\" + \\\"Post-mentral age or PMA(integer)\\\", \\\n",
    "            each string joined by underscore or space.\")\n",
    "    return df\n",
    "\n",
    "def drop_infants_with_less_than_5_after_selecting_infants(data):\n",
    "    '''\n",
    "    Returns data (list of infants) with\n",
    "    infants with more than four samples.\n",
    "    \n",
    "    Input: data (output from combine_cluster_and_abund_data())\n",
    "    \n",
    "    This function is applied after samples\n",
    "    for each timepoint has been selected and DMM.\n",
    "    '''\n",
    "    new_data = []\n",
    "    for infant in data:\n",
    "        if len(list(infant[0].dropna(axis=1)))>=5:\n",
    "            new_data.append(infant)\n",
    "    return new_data\n",
    "\n",
    "def combine_cluster_and_otu_table(df_cluster, df_abund):\n",
    "    '''\n",
    "    Returns list of infants (data). \n",
    "    Each infant contrains 2 DFs:\n",
    "    1st DF: abundance OTU DF\n",
    "    2nd DF: infant's samples 10 timepoints-cluster labels\n",
    "    *Samples with missing data have NaN values for both DFs. \n",
    "    \n",
    "    Inputs:\n",
    "    1. DF output from read_dmm_output() \n",
    "    2. OTU DF (int)\n",
    "    \n",
    "    Column name example: 1000_A_1 (ID_Site_timepoint)\n",
    "    SampleNum: positive integer\n",
    "    Site: single letter\n",
    "    Timepoints: 1-10\n",
    "    '''\n",
    "    samples = df_cluster[\"sample\"]\n",
    "    samples = samples.sort_values()\n",
    "    infants = []\n",
    "    \n",
    "    # find unique ID (sampleID_Site_)\n",
    "    for sample in samples:\n",
    "        infants.append(re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]_\",sample).group())\n",
    "    \n",
    "    unique_infants = sorted(list(set(infants)))\n",
    "    \n",
    "    # create empty dfs:\n",
    "    data = []\n",
    "    for infant in unique_infants:\n",
    "        data.append([pd.DataFrame(columns = [infant+str(x+1) for x in range(10)]),\n",
    "                     pd.DataFrame(columns = [infant+str(x+1) for x in range(10)])])  \n",
    "    \n",
    "    # Store abundance and labels in data.\n",
    "    index = 0\n",
    "    for i,sample in enumerate(samples):\n",
    "        identifier = re.search(\"[0-9]+[_\\-.,/\\s][A-Za-z]_\",sample).group()\n",
    "        cluster = pd.Series(data = [int(df_cluster.loc[df_cluster[\"sample\"]==sample,\"cluster\"])])\n",
    "        if identifier == unique_infants[index]:\n",
    "            data[index][0][sample] = df_abund[sample]\n",
    "            data[index][1][sample]= cluster\n",
    "        elif i==len(samples)-1:\n",
    "            continue\n",
    "        else: \n",
    "            data[index+1][0][sample] = df_abund[sample]\n",
    "            data[index+1][1][sample]= cluster\n",
    "            index+=1\n",
    "    data = drop_infants_with_less_than_5_after_selecting_infants(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "######### Funcitons related to infant_microbiome_hmm_with_labels.py  #########\n",
    "\n",
    "def divide_otu_df_based_on_true_labels(otu_df, labels_df, taxa_names):\n",
    "    '''\n",
    "    not in helperfunctionshmm.py file\n",
    "    divides otu_df into gf and gn otu table absed on labels\n",
    "\n",
    "    *labels_df['Astarte ID'] does not contain all ids in otu_df columns \n",
    "    (some infant samples not present in labels_df)\n",
    "    '''\n",
    "    no_taxa_otu_df = otu_df.drop(columns=['Taxa'])\n",
    "    col_names = list(no_taxa_otu_df) # number_site_PMA\n",
    "    id_list = []\n",
    "    for an_id in col_names:\n",
    "        id_list.append(re.search('[0-9]+',an_id).group())\n",
    "    # *id_list length == column length of no_taxa_otu_df\n",
    "\n",
    "    #loop thorugh labels_df and make 2 dfs based on labels. \n",
    "    # Columns = ['Astarte ID', 'gf']\n",
    "    # ex) id = 1002, label= 0 or 1 \n",
    "    id_and_label = {} \n",
    "    set_id_list = set(id_list) # unique infant id in string\n",
    "    for _, row in labels_df.iterrows():\n",
    "        if str(row.iloc[0]) in set_id_list:\n",
    "            id_and_label[str(row.iloc[0])] = row.iloc[1] \n",
    "            \n",
    "    gf_list_of_sr=[]\n",
    "    gn_list_of_sr=[]\n",
    "\n",
    "    keys = list(id_and_label.keys()) # unique ids\n",
    "    for i, an_id in enumerate(id_list): #same length as otu_df columns\n",
    "        if an_id in keys:\n",
    "            if id_and_label[an_id] == 0:\n",
    "                gn_list_of_sr.append(no_taxa_otu_df.iloc[:,i].T)\n",
    "            if id_and_label[an_id] == 1:\n",
    "                gf_list_of_sr.append(no_taxa_otu_df.iloc[:,i].T)\n",
    "\n",
    "    gn_df = pd.concat(gn_list_of_sr,axis=1)\n",
    "    gf_df = pd.concat(gf_list_of_sr,axis=1)\n",
    "\n",
    "    gn_df.insert(0,'Taxa',taxa_names)\n",
    "    gf_df.insert(0,'Taxa',taxa_names)\n",
    "    return gn_df, gf_df\n",
    "\n",
    "def add_states_to_train_data(data, gn_gf_data):\n",
    "    '''\n",
    "    identify infants in gf_data or gn_data in data,\n",
    "    then adds infant timepoint states (data[infant][1]) to gf_data or gn_data in data.\n",
    "    '''\n",
    "    list_of_all_ids = []\n",
    "    for infant in data:\n",
    "        an_id = re.search('[0-9]+',list(infant[0])[0]).group()\n",
    "        list_of_all_ids.append(an_id)\n",
    "        \n",
    "    for infant in gn_gf_data:\n",
    "        an_id = re.search('[0-9]+',list(infant[0])[0]).group()\n",
    "        if an_id in set(list_of_all_ids):\n",
    "            idx = list_of_all_ids.index(an_id)\n",
    "            infant.append(data[idx][1])\n",
    "    \n",
    "    #find infant with state data\n",
    "    new_data = []\n",
    "    for infant in gn_gf_data:\n",
    "        if len(infant)==2:\n",
    "            new_data.append(infant)\n",
    "            \n",
    "    return new_data\n",
    "\n",
    "def divide_based_on_true_labels(otu_df, taxa_names, label_filepath):\n",
    "    '''\n",
    "    reads in label.tsv\n",
    "    divides original data based on label\n",
    "    '''\n",
    "    labels_df = pd.read_csv(label_filepath, delimiter='\\t')\n",
    "    gn_df, gf_df = divide_otu_df_based_on_true_labels(otu_df, labels_df, taxa_names)\n",
    "    return gn_df, gf_df\n",
    "    \n",
    "def convert_gn_gf_df_to_data_and_add_states_from_dmm(gn_df, gf_df, data):\n",
    "        gn_data = convert_otu_df_to_data(gn_df,include_nan=True)\n",
    "        gf_data = convert_otu_df_to_data(gf_df,include_nan=True)\n",
    "        gn_data = add_states_to_train_data(data, gn_data)\n",
    "        gf_data = add_states_to_train_data(data, gf_data)\n",
    "        return gn_data, gf_data\n",
    "\n",
    "######### Funcitons related to Initial probabilties  #########\n",
    "\n",
    "def logdiffexp(la, lb):\n",
    "    '''\n",
    "    Returns log of difference \n",
    "    of exponent values of la \n",
    "    and lb. Smaller value is \n",
    "    subtracted from bigger value, \n",
    "    so return value is always positive.\n",
    "    \n",
    "    Inputs: two values in \n",
    "    natural log space.\n",
    "    \n",
    "    *When using this function, an \n",
    "    exception cases where la and lb values are \n",
    "    the same is required! Those exceptions are\n",
    "    not handled in this function.\n",
    "    '''\n",
    "    if lb > la:\n",
    "        tmp = la\n",
    "        la = lb\n",
    "        lb = tmp\n",
    "        \n",
    "    if (lb - la < -0.693):\n",
    "        return la + np.log1p(-np.exp(lb - la, dtype=np.float128), dtype=np.float128)\n",
    "    else:\n",
    "        return la + np.log(-np.expm1(lb - la, dtype=np.float128), dtype=np.float128)\n",
    "    \n",
    "def generate_initial_matrix(data, n_states = 6):\n",
    "    '''\n",
    "    Returns initial probabilities of each first_state as pd.Series.\n",
    "    \n",
    "    Input: \n",
    "    1.data (output from combine_cluster_and_abund_data())\n",
    "    2.n_states (number of states or n_clusters) (natural number). This must match \n",
    "    the number of states put in for DMM.\n",
    "    \n",
    "    States in data must be labeled with natural numbers. No floats.\n",
    "    '''\n",
    "    list_of_first_pos = np.array([], dtype=np.float128)\n",
    "    for infant in data:\n",
    "        no_nan_ser = infant[1].iloc[0,:].dropna()\n",
    "        list_of_first_pos = np.append(list_of_first_pos, no_nan_ser[0])\n",
    "    \n",
    "    tot_cnt = len(list_of_first_pos)\n",
    "    ser = pd.Series(list_of_first_pos).value_counts()\n",
    "    init = ser.sort_index() \n",
    "    \n",
    "    if len(init) == n_states:        \n",
    "        x = (np.log(init / tot_cnt, dtype=np.float128))\n",
    "        return x.to_numpy()\n",
    "    else:\n",
    "        states = [i+1 for i in range(n_states)]\n",
    "        excluded_states = []\n",
    "        for s1 in states:\n",
    "            if int(s1) not in init.index:\n",
    "                excluded_states.append(int(s1))\n",
    "        init = pd.concat([init,pd.Series([0]*len(excluded_states),index=excluded_states, dtype=\"float128\")])\n",
    "        init += 1\n",
    "        init = init / (tot_cnt+n_states)\n",
    "        init.sort_index(inplace=True)\n",
    "        init = np.log(init, dtype=np.float128)\n",
    "        return init.to_numpy()\n",
    "\n",
    "######### Funcitons related to transition probabilties  #########\n",
    "\n",
    "def generate_transition_matrix(data, n_states=6):\n",
    "    '''\n",
    "    Returns n_states by n_states transition matrix\n",
    "    \n",
    "    Input: \n",
    "    1. data (output from combine_cluster_and_abund_data())\n",
    "    2.n_states (number of states or n_clusters). This \n",
    "    must match the number of states put in for DMM.\n",
    "    '''        \n",
    "    tot_cnt = np.zeros((n_states,), dtype=np.float128)\n",
    "    m = np.zeros((n_states,n_states), dtype=np.float128)\n",
    "    \n",
    "    for sample in data:\n",
    "        states_sr = sample[1].iloc[0,:]\n",
    "        no_nan_sr = list(states_sr.dropna())\n",
    "        \n",
    "        for j,state in enumerate(no_nan_sr):\n",
    "            #total count of each state upto last element\n",
    "            if j != len(no_nan_sr)-1:\n",
    "                tot_cnt[state-1]+=1 # 1st position of moving window of 2 (doesn't count last element)\n",
    "            if j != 0:\n",
    "                m[no_nan_sr[j-1]-1][state-1]+=1 # 2nd position of moving window of 2 (doesn't count first element)\n",
    "    if 0 in m:\n",
    "        #add 1 for smoothing\n",
    "        m += 1 \n",
    "        tot_cnt += n_states\n",
    "        for i in range(n_states):\n",
    "            m[i] = m[i] / tot_cnt[i]\n",
    "    else:\n",
    "        for i in range(n_states):\n",
    "            m[i] = m[i] / tot_cnt[i]\n",
    "    m = np.log(m, dtype=np.float128)\n",
    "    return m\n",
    "\n",
    "######### Funcitons related to Emission probabilties  #########\n",
    "\n",
    "def divide_observations_based_on_states(data, n_states=6):\n",
    "    '''\n",
    "    Returns a DF with columns of \n",
    "    means and variances of each taxa\n",
    "    of each hidden state.\n",
    "    \n",
    "    Input: \n",
    "    1. data (output from combine_cluster_and_abund_data())\n",
    "    2. n_states (number of states or n_clusters). \n",
    "    This must match the number of states put in for DMM.\n",
    "    '''\n",
    "    # Make 6 empty DFs\n",
    "    obs_based_on_states = []\n",
    "    for i in range(n_states):\n",
    "        obs_based_on_states.append(pd.DataFrame())\n",
    "    \n",
    "    # Get abundance data based on the hidden states\n",
    "    for infant in data: #240 samples\n",
    "        for i, col_name in enumerate(list(infant[1].columns)):\n",
    "            smp_state = infant[1].loc[0,col_name]\n",
    "            if np.isnan(smp_state): #handle nan values\n",
    "                continue\n",
    "            obs_based_on_states[smp_state-1] = pd.concat([obs_based_on_states[smp_state-1],infant[0][col_name]],axis=1)\n",
    "\n",
    "    return obs_based_on_states\n",
    "\n",
    "def calc_mean_var_of_each_state(obs_based_on_states, n_states = 6): \n",
    "    '''\n",
    "    Returns DF of mu and var of all states.\n",
    "    shape=(12,n_taxa)\n",
    "    \n",
    "    Input:\n",
    "    1. output from divide_observations_based_on_states():\n",
    "    list of abundance DFs divded based on \n",
    "    hidden state.\n",
    "    2. n_states (number of states or n_clusters). This \n",
    "    must match the number of states put in for DMM.\n",
    "    Cacluates mean and variance of each state.\n",
    "    Mean and variance in df_e_m are not in log space.\n",
    "    '''\n",
    "    \n",
    "    # make empty DF for means and variances of each taxon\n",
    "    col_names = []\n",
    "    for i in range(n_states):\n",
    "        col_names.append(\"mu\"+str(i+1))\n",
    "        col_names.append(\"var\"+str(i+1))\n",
    "    df_e_m = pd.DataFrame(columns = col_names)\n",
    "    df_e_m.fillna(0,inplace=True)\n",
    "    \n",
    "    #check if all abund values are the same\n",
    "    for i,df in enumerate(obs_based_on_states):\n",
    "        np_version = df.to_numpy()\n",
    "        row_val_all_same = False\n",
    "        for row in np_version:\n",
    "            if (row[0]==row).all():\n",
    "                row_val_all_same = True\n",
    "        if row_val_all_same == True:\n",
    "            df['tmp']= [1.1]*len(df)\n",
    "    \n",
    "    # calculate means and variances and store them in df_e_m\n",
    "    for i,df in enumerate(obs_based_on_states):\n",
    "        sample_cnt = len(list(df))\n",
    "        sum_sr = df.sum(axis = 1)\n",
    "        mu_sr = sum_sr.divide(sample_cnt)\n",
    "        df_e_m[\"mu\"+str(i+1)] = mu_sr\n",
    "        \n",
    "        var = []\n",
    "        for j in range(len(df)): #loop through rows of df\n",
    "            deviations_sum = ((df.iloc[j,:] - mu_sr.iloc[j])**2).sum() \n",
    "            var.append(deviations_sum / sample_cnt)\n",
    "        df_e_m[\"var\"+str(i+1)] = var\n",
    "    return df_e_m\n",
    "\n",
    "def e_m_to_np(df_e_m):\n",
    "    '''\n",
    "    Returns np version of emission matrix \n",
    "    (output of calc_mean_var_of_each_state())\n",
    "    Shape = (n_states,2,n_taxa)\n",
    "    \n",
    "    Input:output of calc_mean_var_of_each_state()\n",
    "    \n",
    "    *Output means and variances are in natural log space.\n",
    "    '''\n",
    "    n_states = int(len(list(df_e_m))/2)\n",
    "    n_taxa = len(df_e_m)\n",
    "    np_e_m = np.zeros((n_states,2,n_taxa), dtype=np.float128)\n",
    "    \n",
    "    for col in list(df_e_m):\n",
    "        state = int(re.search('[0-9]+',col).group())-1\n",
    "        if col[:2]==\"mu\":\n",
    "            np_e_m[state,0] = df_e_m[col].to_numpy()\n",
    "        else:\n",
    "            np_e_m[state,1] = df_e_m[col].to_numpy()\n",
    "    return np.log(np_e_m, dtype=np.float128)\n",
    "\n",
    "def normal_dist_PDF(abund , mean , var):\n",
    "    '''\n",
    "    Returns probability calculated using \n",
    "    normal distribution pdf of the abundance \n",
    "    value in log space (natural log). \n",
    "    \n",
    "    Input:\n",
    "    1. abund (abundance value) must \n",
    "    not be in log space. \n",
    "    2.mean must be in log space. \n",
    "    3.var (variance) must be in log space. \n",
    "    '''\n",
    "    sd = np.multiply(0.5,var,dtype=np.float128) \n",
    "    a = - (sd + 0.5*np.log(2 * np.pi))\n",
    "    if np.round(np.log(abund, dtype=np.float128),decimals=4) != np.round(mean,decimals=4):\n",
    "        b = - np.exp((np.log(0.5, dtype=np.float128) + (2*(logdiffexp(np.log(abund,dtype=np.float128),mean)-sd))), dtype=np.float128)\n",
    "        return a+b\n",
    "    else:\n",
    "        return a\n",
    "    \n",
    "def calc_emis_prob(e_m, sample, state):\n",
    "    '''\n",
    "    Returns emission probability\n",
    "    of a sample in log space.\n",
    "    Input:\n",
    "    1. emission matrix (e_m)\n",
    "    2. sample (shape=(1 x 444))\n",
    "    3. state the sample is in.\n",
    "    State value begins from 0 for this function, not 1.\n",
    "    '''\n",
    "    n_taxa = len(sample)\n",
    "    prob = np.array([], dtype=np.float128)\n",
    "    for i in range(n_taxa):\n",
    "        mean = e_m[state,0,i]\n",
    "        var = e_m[state,1,i]\n",
    "        prob = np.append(prob, normal_dist_PDF(sample.iloc[i],mean,var))\n",
    "    ans = np.sum(prob, dtype=np.float128)\n",
    "    return ans\n",
    "\n",
    "######### Funcitons for  calculating alpha / beta (e step) #########\n",
    "\n",
    "def calc_alpha(infant, i_m, t_m, e_m, n_states=6):\n",
    "    '''\n",
    "    Returns calculated alpha variables.\n",
    "    \n",
    "    Input: infant, which is a list of DFs \n",
    "    ([0] = abundance data, [1] = states)\n",
    "    \n",
    "    Output matrix is in natural log space.\n",
    "    '''\n",
    "    abund = infant[0]\n",
    "    timepoint = len(list(infant[0]))\n",
    "\n",
    "    alpha = np.zeros((n_states,timepoint), dtype=np.float128)\n",
    "    \n",
    "    for tp in range(timepoint):\n",
    "        sample = abund.iloc[:,tp]\n",
    "        for i in range(n_states):\n",
    "            if tp == 0:\n",
    "                if np.isnan(sample.iloc[0]):\n",
    "                    alpha[i,tp] = i_m[i]\n",
    "                else:\n",
    "                    alpha[i,tp] =  calc_emis_prob(e_m, sample, i) + i_m[i]\n",
    "            else:\n",
    "                a_i_j = []\n",
    "                a_i = 0.0\n",
    "                for j in range(n_states): \n",
    "                    if np.isnan(sample.iloc[0]):\n",
    "                        a_i_j.append(t_m[j,i] + alpha[j,tp-1])\n",
    "                    else:\n",
    "                        a_i_j.append(calc_emis_prob(e_m,sample,i) + t_m[j,i] + alpha[j,tp-1])\n",
    "                a_i = logsumexp(a_i_j)\n",
    "                alpha[i,tp] = a_i\n",
    "    return alpha\n",
    "\n",
    "def calc_beta(infant, i_m, t_m, e_m, n_states=6):\n",
    "    '''\n",
    "    Returns calculated beta variables.\n",
    "    \n",
    "    Input: infant, which is a list of DFs \n",
    "    ([0] = abundance data, [1] = states)\n",
    "    \n",
    "    Output matrix is in natural log space.\n",
    "    '''\n",
    "    abund = infant[0]\n",
    "    timepoint = len(list(infant[1]))\n",
    "    beta = np.zeros((n_states,timepoint), dtype=np.float128)\n",
    "    beta[:,-1] = 0 #inialize last timepoint as 0 (because calculating beta in log space)\n",
    "    \n",
    "    for tp in range(timepoint-2,-1,-1): # timepoint loop => loop through 8~0 (not 9~1)\n",
    "        sample = abund.iloc[:,tp+1]\n",
    "        for i in range(n_states): # i loop (transition mtx idx)\n",
    "            b_i_j = []\n",
    "            for j in range(n_states): # j loop (beta / transition mtx idx)\n",
    "                if np.isnan(sample.iloc[0]):\n",
    "                    b_i_j.append(t_m[i,j] + beta[j,tp+1])\n",
    "                else:\n",
    "                    b_i_j.append(t_m[i,j] + beta[j,tp+1] + calc_emis_prob(e_m,sample,j))\n",
    "            b_i = logsumexp(b_i_j)\n",
    "            beta[i,tp] = b_i\n",
    "    return beta\n",
    "\n",
    "\n",
    "def calc_alphas_and_betas(data, i_m, t_m, e_m, n_states=6):\n",
    "    '''\n",
    "    Returns data with updated alpha & beta matricies for all infants.\n",
    "    \n",
    "    Input:\n",
    "    1. data (output from combine_cluster_and_abund_data())\n",
    "    2. i_m (np.array initial matrix from generate_initial_matrix())\n",
    "    3. t_m (np.array transition matrix from generate_transition_matrix())\n",
    "    4. t_m (np.array emission matrix from e_m_to_np())\n",
    "    5. n_states (number of states or n_clusters). \n",
    "    This must match the number of states put in for DMM.\n",
    "    \n",
    "    This function either calculates or updates alpha and beta \n",
    "    matrices at index 2 and 3 for all infants in data. \n",
    "    '''\n",
    "    for infant in data:\n",
    "        if len(infant) == 2: #1st e step of EM\n",
    "            infant.append(calc_alpha(infant,i_m, t_m, e_m))\n",
    "            infant.append(calc_beta(infant,i_m, t_m, e_m))\n",
    "        elif len(infant) == 3:\n",
    "            infant[2] = calc_alpha(infant,i_m, t_m, e_m)\n",
    "            infant.append(calc_beta(infant,i_m, t_m, e_m))\n",
    "        else: # updating on steps after the first\n",
    "            infant[2] = calc_alpha(infant,i_m, t_m, e_m)\n",
    "            infant[3] = calc_beta(infant,i_m, t_m, e_m)\n",
    "    return data\n",
    "\n",
    "######### Funcitons for calculating st_i / st_i_j for e step #########\n",
    "\n",
    "def calc_st_i(infant):\n",
    "    '''\n",
    "    Returns calculated probability matrix of state i \n",
    "    at timepoint t given the observation: s_t(i)\n",
    "    Shape = (n_states x timepoints)\n",
    "    \n",
    "    Input: infant, which is a list of DFs:\n",
    "    [0] = abundance data, [1] = states at 10 timepoints, [2] =alpha matrix, [3] = beta matrix\n",
    "\n",
    "    Output matrix is in log space.\n",
    "    '''\n",
    "    # Calculate S_t(i) of an infant\n",
    "    n_states = len(infant[2]) \n",
    "    n_timepoints = len(infant[2][0])\n",
    "    alpha = infant[2]\n",
    "    beta = infant[3]\n",
    "    \n",
    "    st_i = np.zeros((n_states,n_timepoints), dtype=np.float128)\n",
    "    \n",
    "    for tp in range(n_timepoints):\n",
    "        sum_of_prod_of_ab_all_states = np.array([],dtype=np.float128) #설마....... # denom\n",
    "        sum_of_prod_of_ab_all_states = logsumexp(alpha[:,tp] + beta[:,tp]) # sum of alpha*beta (all states) of each timepoint (denom)\n",
    "        for i in range(n_states):\n",
    "            st_i[i,tp] = alpha[i,tp] + beta[i,tp]\n",
    "        st_i[:,tp] = st_i[:,tp] - sum_of_prod_of_ab_all_states\n",
    "    return st_i\n",
    "\n",
    "def calc_st_i_j(infant, t_m, e_m):\n",
    "    '''\n",
    "    Returns calculated transition matrices \n",
    "    of each timepoint given the observation: s_t(i,j)\n",
    "    Shape = (n_timepoints-1, n_states, n_states)\n",
    "    \n",
    "    Input: infant, which is a list of DFs:\n",
    "    [0] = abundance data, [1] = states at 10 timepoints, [2] =alpha matrix, [3] = beta matrix\n",
    "\n",
    "    Output matrices are in log space.\n",
    "    '''\n",
    "    n_states = len(infant[2])\n",
    "    n_timepoints = len(infant[2][0])\n",
    "    abund_data = infant[0]\n",
    "    alpha = infant[2]\n",
    "    beta = infant[3]\n",
    "    \n",
    "    st_i_j_list=[]\n",
    "    for tp in range(n_timepoints-1): # loop through 0-8 (not 0-9)\n",
    "        st_i_j = np.zeros((n_states,n_states), dtype=np.float128) # n_states by n_states\n",
    "        \n",
    "        sum_of_prod_of_ab_all_states = np.array([],dtype=np.float128)\n",
    "        sum_of_prod_of_ab_all_states = logsumexp(alpha[:,tp] + beta[:,tp]) # sum of alpha*beta (all states) of each timepoint (denom)\n",
    "        for i in range(n_states):\n",
    "            for j in range(n_states):\n",
    "                if np.isnan(abund_data.iloc[0,tp+1]):\n",
    "                    st_i_j[i,j] = alpha[i,tp] + beta[j,tp+1] + t_m[i,j]\n",
    "                else:\n",
    "                    st_i_j[i,j] = alpha[i,tp] + beta[j,tp+1] + t_m[i,j] + calc_emis_prob(e_m, abund_data.iloc[:,tp+1], j)\n",
    "            st_i_j[i,:] = st_i_j[i,:] - sum_of_prod_of_ab_all_states\n",
    "        \n",
    "        st_i_j_list.append(st_i_j)\n",
    "    return st_i_j_list\n",
    "\n",
    "def calc_sts(data, t_m, e_m):\n",
    "    '''\n",
    "    Returns data with updated st_i & st_ij matricies for all infants.\n",
    "    \n",
    "    Input:\n",
    "    1. data (output from combine_cluster_and_abund_data())\n",
    "    2. t_m (np.array transition matrix from generate_transition_matrix())\n",
    "    3. t_m (np.array emission matrix from e_m_to_np())\n",
    "    4. n_states (number of states or n_clusters). This must \n",
    "    match the number of states put in for DMM.\n",
    "    \n",
    "    This function either calculates or updates st_i & st_ij\n",
    "    matrices at index 4 and 5 for all infants in data.\n",
    "    '''\n",
    "    for infant in data:\n",
    "        if len(infant) == 4:\n",
    "            infant.append(calc_st_i(infant))\n",
    "            infant.append(calc_st_i_j(infant, t_m, e_m))\n",
    "        elif len(infant) == 5:\n",
    "            infant[4] = calc_st_i(infant)\n",
    "            infant.append(calc_st_i_j(infant, t_m, e_m))\n",
    "        else:\n",
    "            infant[4] = calc_st_i(infant)\n",
    "            infant[5] = calc_st_i_j(infant, t_m, e_m)\n",
    "    return data\n",
    "\n",
    "######### Funcitons for  calculating i_m, t_m, e_m for m step #########\n",
    "\n",
    "def m_step_initial_matrix(data):\n",
    "    '''\n",
    "    Returns initial matrix.\n",
    "    Shape = (n_states,)\n",
    "    \n",
    "    Input: updated data with alpha, beta, sti and stij.\n",
    "    \n",
    "    Generates initial probability matrix\n",
    "    using all s_t(i) matricies in data.\n",
    "    \n",
    "    Output matrix is in log space.\n",
    "    '''\n",
    "    n_states = len(data[0][2]) #n_rows of alpha matrix\n",
    "    i_m = np.array([],dtype=np.float128)\n",
    "    all_states_tot_sum = np.float128(0) # 의미가 있나?\n",
    "    \n",
    "    for x, infant in enumerate(data):\n",
    "        sti = infant[4]\n",
    "        tp_1_sum = logsumexp(sti[:,0]) #sum of 1st column\n",
    "        if x == 0:\n",
    "            i_m = sti[:,0]\n",
    "            all_states_tot_sum = tp_1_sum\n",
    "        else:\n",
    "            for i in range(n_states):\n",
    "                i_m[i] = logsumexp([i_m[i],sti[i,0]])\n",
    "            all_states_tot_sum = logsumexp([all_states_tot_sum,tp_1_sum])\n",
    "    \n",
    "    i_m = i_m - all_states_tot_sum\n",
    "    return i_m\n",
    "            \n",
    "def m_step_transition_matrix(data):\n",
    "    '''\n",
    "    Returns transition matrix.\n",
    "    Shape = (n_states, n_states)\n",
    "    \n",
    "    Input: updated data with alpha, beta, sti and stij.\n",
    "    \n",
    "    Generates transition probabilities\n",
    "    using all s_t(i,j) matricies in data.\n",
    "    \n",
    "    Output matrix is in log space\n",
    "    '''\n",
    "    n_states = len(data[0][2]) #n_rows of alpha matrix == number of states\n",
    "    t_m = np.array([],dtype=np.float128)\n",
    "    all_states_tot_sum = np.zeros((n_states), dtype=np.float128) # tot sum of probabilistic count of each state\n",
    "    \n",
    "    for x, infant in enumerate(data):\n",
    "        for tp, stij in enumerate(infant[5]):\n",
    "            for i in range(n_states):\n",
    "                row_sum = logsumexp(stij[i,:])\n",
    "                if x == 0 and tp == 0:\n",
    "                    all_states_tot_sum[i] = row_sum\n",
    "                    if i == n_states-1: # x==0 , tp ==0 일때 stij 한번만 넣어주기위해 마지막에 state에 넣음.\n",
    "                        t_m = stij\n",
    "                else:\n",
    "                    all_states_tot_sum[i] = logsumexp([all_states_tot_sum[i],row_sum])\n",
    "                    for j in range(n_states):\n",
    "                        t_m[i,j] = logsumexp([t_m[i,j],stij[i,j]])\n",
    "    for s in range(n_states):\n",
    "        t_m[s] = t_m[s] - all_states_tot_sum[s]\n",
    "    return t_m\n",
    "\n",
    "def m_step_emission_matrix(data):\n",
    "    '''\n",
    "    Returns emission matrix\n",
    "    Shape=(n_states,2,n_taxa)\n",
    "    At axis=1, index 0 is mu and 1 is variance.\n",
    "    \n",
    "    Input: updated data with alpha, beta, sti and stij.\n",
    "    \n",
    "    Mu and variances are calculated using \n",
    "    abundance and s_t(i) matricies in updated data. \n",
    "    '''\n",
    "    n_states = len(data[0][2]) #n_rows of alpha matrix == number of states\n",
    "    timepoint = len(data[0][2][0])\n",
    "    n_taxa = len(data[0][0])\n",
    "    n_infants = len(data)\n",
    "    \n",
    "    #compile all data in stack_of_states\n",
    "    stack_of_states = np.array([], dtype=np.float128) # shape =(n_state, n_infant, n_taxa)\n",
    "    \n",
    "    denom_sum_of_weights = np.zeros((6), dtype=np.float128)\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        stack_of_samples = np.array([], dtype=np.float128) #exists for each state\n",
    "        \n",
    "        for idx, infant in enumerate(data):\n",
    "            abund = infant[0]\n",
    "            sti = infant[4]\n",
    "            for tp in range(timepoint):\n",
    "                if np.isnan(abund.iloc[0,tp]) == False: # if abund value not nan\n",
    "                    if denom_sum_of_weights[state] == 0:\n",
    "                        denom_sum_of_weights[state] = sti[state,tp]\n",
    "                    else:\n",
    "                        denom_sum_of_weights[state] = logsumexp([denom_sum_of_weights[state], sti[state,tp]])\n",
    "                    if idx == 0:\n",
    "                        stack_of_samples = np.expand_dims( (np.log(abund.iloc[:,tp], dtype=np.float128) + sti[state,tp]), 0) #becomes 2D (stack_of_samples)\n",
    "                    else:\n",
    "                        stack_of_samples = np.append(stack_of_samples, np.expand_dims( (np.log(abund.iloc[:,tp], dtype=np.float128) + sti[state,tp]), 0), axis=0)\n",
    "        \n",
    "        if state == 0: #becomes 3D (stack_of_states)\n",
    "            stack_of_states = np.expand_dims(stack_of_samples,0)\n",
    "        else:\n",
    "            stack_of_states = np.append(stack_of_states, np.expand_dims(stack_of_samples,0),axis=0) \n",
    "            \n",
    "    #compute mu and sigma of each state and store them in e_m\n",
    "    e_m = np.zeros((n_states, 2, n_taxa), dtype=np.float128) # (6, 2, 444)\n",
    "    \n",
    "    \n",
    "    for i in range(n_states):\n",
    "        for j in range(n_taxa):\n",
    "            weighted_taxa_data_at_state_i = stack_of_states[i,:,j]\n",
    "            #calc mu \n",
    "            mu = logsumexp(weighted_taxa_data_at_state_i) - denom_sum_of_weights[i]\n",
    "            e_m[i,0,j] = mu\n",
    "            \n",
    "            #calc var\n",
    "            cnt_when_abund_same_as_mu = 0\n",
    "            deviations = np.array([], dtype=np.float128)\n",
    "            for abund_val in weighted_taxa_data_at_state_i:\n",
    "                if np.round(abund_val,decimals=4) != np.round(mu,decimals=4):\n",
    "                    deviations = np.append(deviations, 2*logdiffexp(abund_val, mu))\n",
    "                else:\n",
    "                    cnt_when_abund_same_as_mu+=1\n",
    "            e_m[i,1,j] = logsumexp(deviations) - np.log((len(deviations)+cnt_when_abund_same_as_mu), dtype=np.float128)\n",
    "            \n",
    "    return e_m\n",
    "\n",
    "######### hmm em algorithm #########\n",
    "def e_step(data,i_m,t_m,e_m,states=6):\n",
    "    '''\n",
    "    Inference step\n",
    "    \n",
    "    Returns updated data:\n",
    "    \n",
    "    Input:\n",
    "    1. data\n",
    "    2. i_m (np.array initial matrix)\n",
    "    3. t_m (np.array transition matrix)\n",
    "    4. t_m (np.array emission matrix)\n",
    "    5. n_states (This must match the number of states put in for DMM.)\n",
    "    \n",
    "    After e step, all infants contains:\n",
    "    [0] = abundance data, [1] = states at 10 timepoints, [2] =alpha matrix, [3] = beta matrix\n",
    "    '''\n",
    "    updated_data = calc_alphas_and_betas(data,i_m, t_m, e_m, n_states=states)\n",
    "    updated_data = calc_sts(updated_data, t_m, e_m)\n",
    "    return updated_data\n",
    "\n",
    "def m_step(data):\n",
    "    '''\n",
    "    MLE step\n",
    "    \n",
    "    Returns initial, transition and emission matrix.\n",
    "    \n",
    "    Input:data updated in e step.\n",
    "    \n",
    "    '''\n",
    "    i_m = m_step_initial_matrix(data)\n",
    "    t_m = m_step_transition_matrix(data)\n",
    "    e_m = m_step_emission_matrix(data)\n",
    "    return i_m, t_m, e_m\n",
    "\n",
    "def find_viterbi_path(data, i_m, t_m, e_m):\n",
    "    '''\n",
    "    Return updated data appended with highest probability path of states for all infants.\n",
    "    use this func after e and m step \n",
    "    to find the most probable path.\n",
    "    \n",
    "    Viterbi path of all infants calculated\n",
    "    using i_m, t_m, e_m.\n",
    "    \n",
    "    Returned v_path_of_states state starts from index 0 not 1.\n",
    "    '''\n",
    "    n_states = len(i_m)\n",
    "    n_timepoints = len(list(data[0][0]))\n",
    "    \n",
    "    v_m = np.zeros((n_states,n_timepoints),dtype=np.float128)\n",
    "    \n",
    "    for infant in data:\n",
    "        \n",
    "        v_path_of_states = np.array([],dtype=np.int8) #np.int8 => -128 ~ 127 \n",
    "        abund = infant[0]\n",
    "        for tp in range(n_timepoints):\n",
    "            sample = abund.iloc[:,tp]\n",
    "            prev_tp_max = -1e100 #initialize prev_tp_max with a negative every timepoint\n",
    "            prev_tp_max_idx = -1\n",
    "            for i in range(n_states):\n",
    "                if tp==0:\n",
    "                    if np.isnan(sample.iloc[0]):\n",
    "                        v_m[i,tp] = i_m[i]\n",
    "                    else:\n",
    "                        v_m[i,tp] = calc_emis_prob(e_m,sample,i) + i_m[i]\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        for j in range(n_states):\n",
    "                            if v_m[j,tp-1] > prev_tp_max:\n",
    "                                prev_tp_max = v_m[j,tp-1]\n",
    "                                prev_tp_max_idx = j\n",
    "                        v_path_of_states = np.append(v_path_of_states, int(prev_tp_max_idx))\n",
    "                    if np.isnan(sample.iloc[0]):\n",
    "                        v_m[i,tp] = t_m[prev_tp_max_idx,i] + prev_tp_max\n",
    "                    else:\n",
    "                        v_m[i,tp] = calc_emis_prob(e_m,sample,i) + t_m[prev_tp_max_idx,i] + prev_tp_max\n",
    "            if tp == n_timepoints-1:\n",
    "                last_tp_max = -1e100\n",
    "                last_tp_max_idx = -1\n",
    "                for j in range(n_states):\n",
    "                    if v_m[j,tp] > last_tp_max:\n",
    "                        last_tp_max = v_m[j,tp]\n",
    "                        last_tp_max_idx = j\n",
    "                v_path_of_states = np.append(v_path_of_states, int(last_tp_max_idx))\n",
    "        \n",
    "        if len(infant) == 6:\n",
    "            infant.append(v_path_of_states)\n",
    "        elif len(infant) == 7:\n",
    "            infant[6] = v_path_of_states\n",
    "        elif len(infant) > 7:\n",
    "            print('Infant length bigger than 7')\n",
    "        elif len(infant) <6:\n",
    "            print('Infant length smaller than 6. Infant is missing one or more of the following: alpha, beta, s_t(i) or s_t(i,j)')\n",
    "    return data\n",
    "\n",
    "def calc_loglikelihood_viterbi(data,i_m,t_m,e_m):\n",
    "    '''\n",
    "    Returns loglikelihood of hmm model,\n",
    "    calculated using viterbi path.\n",
    "    \n",
    "    Input: data updated in e step\n",
    "    '''\n",
    "    n_states = len(i_m)\n",
    "    n_timepoints = len(list(data[0][0]))\n",
    "    likelihood = np.array([],dtype=np.float128)\n",
    "    \n",
    "    for infant in data:\n",
    "        abund = infant[0]\n",
    "        v_path = infant[6]\n",
    "        for tp in range(n_timepoints):\n",
    "            state = v_path[tp]\n",
    "            sample = abund.iloc[:,tp]\n",
    "            if tp == 0:\n",
    "                if np.isnan(sample.iloc[0]):\n",
    "                    likelihood = np.append(likelihood, i_m[state])\n",
    "                else:\n",
    "                    likelihood = np.append(likelihood, i_m[state] + calc_emis_prob(e_m,sample,state))\n",
    "            else:\n",
    "                prev_state = v_path[tp-1]\n",
    "                \n",
    "                if np.isnan(sample.iloc[0]):\n",
    "                    likelihood = np.append(likelihood, t_m[prev_state, state])\n",
    "                else:\n",
    "                    likelihood = np.append(likelihood, t_m[prev_state, state] + calc_emis_prob(e_m,sample,state))\n",
    "                    \n",
    "    return np.sum(likelihood, dtype=np.float128)\n",
    "\n",
    "def calc_percent_diff(likelihoods):\n",
    "    '''\n",
    "    Returns percent difference of \n",
    "    last and second to last element \n",
    "    of likelihoods list.\n",
    "    \n",
    "    Input: list of likelihoods\n",
    "    \n",
    "    *len(likelihoods) must be > 2\n",
    "    '''\n",
    "    last_idx = len(likelihoods)-1\n",
    "    bf = likelihoods[last_idx-1]\n",
    "    af = likelihoods[last_idx]\n",
    "    \n",
    "    return abs((af-bf)/(af+bf)/2)*100\n",
    "\n",
    "def em_algo(data, init_i_m, init_t_m, init_e_m, threshold):\n",
    "    '''\n",
    "    Returns caclulated likelihoods (list), \n",
    "    final initial matrix, final transition matrix \n",
    "    and final emission matrix.\n",
    "\n",
    "    Input:\n",
    "    1. data (output from combine_cluster_and_otu_table())\n",
    "    2. init_i_m (output from generate_initial_matrix())\n",
    "    3. init_t_m (output from generate_transition_matrix())\n",
    "    4. init_e_m (output from e_m_to_np())\n",
    "    5. threshold (threshold parameter is compared with the \n",
    "    percent difference of the current and previous log likelihood.)\n",
    "\n",
    "    *This function prints percent difference of each step.\n",
    "    *If the iteration goes over 20, it loops out and converges.\n",
    "    '''\n",
    "    print('Threshold: ',threshold)\n",
    "    print()\n",
    "    likelihoods = np.array([],dtype=np.float128)\n",
    "    i_m, t_m, e_m = np.array([], dtype=np.float128), np.array([], dtype=np.float128), np.array([], dtype=np.float128)\n",
    "    percent_diff = 123456789 #initialize with big number\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        if  percent_diff < threshold:\n",
    "            break\n",
    "        elif i == 20:\n",
    "            break\n",
    "        if i == 0:\n",
    "            data = e_step(data, init_i_m, init_t_m, init_e_m)\n",
    "            i_m, t_m, e_m = m_step(data)\n",
    "            data = find_viterbi_path(data, i_m, t_m, e_m)\n",
    "            likelihoods = np.append(likelihoods, calc_loglikelihood_viterbi(data,i_m, t_m, e_m))\n",
    "        else: \n",
    "            data = e_step(data, i_m, t_m, e_m)\n",
    "            i_m, t_m, e_m = m_step(data)\n",
    "            data = find_viterbi_path(data, i_m, t_m, e_m)\n",
    "            likelihoods = np.append(likelihoods, calc_loglikelihood_viterbi(data,i_m, t_m, e_m))\n",
    "            \n",
    "            percent_diff = calc_percent_diff(likelihoods)\n",
    "        i+=1\n",
    "        \n",
    "        if i == 1:\n",
    "            print('============================ Iteration: ',i)\n",
    "            print('Current Likelihood: ', likelihoods[len(likelihoods)-1])\n",
    "            print()\n",
    "        else:\n",
    "            print('============================ Iteration: ',i)\n",
    "            print('Current Likelihood: ', likelihoods[len(likelihoods)-1])\n",
    "            print('Likelihood percent difference: ', percent_diff)\n",
    "            print()\n",
    "            if likelihoods[len(likelihoods)-2] > likelihoods[len(likelihoods)-1] and percent_diff > 0.05:\n",
    "                print(\"likelihood dropped more than 0.05. Exclude the current likelihood that dropped.\")\n",
    "                return likelihoods[:len(likelihoods)-2], i_m, t_m, e_m\n",
    "\n",
    "    return likelihoods, i_m, t_m, e_m\n",
    "\n",
    "def output_likelihood_graph(score, filepath):\n",
    "    '''\n",
    "    Saves a graph of likelihood \n",
    "    score in current working directory.\n",
    "\n",
    "    Input: score (list of likelihood values)\n",
    "    '''\n",
    "    plt.figure(dpi=150)\n",
    "    plt.rc(\"font\", size=6.5)\n",
    "    plt.plot(range(0,len(score)), score, color = \"black\", marker = \"o\")\n",
    "    plt.title(\"Log Likelihood\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Log Likelihood\")\n",
    "    plt.savefig(filepath+\"/hmm_log_likelihood.png\")\n",
    "    plt.close(None)\n",
    "    print('HMM log likelihood graph saved as: hmm_log_likelihood.png')\n",
    "\n",
    "def output_i_t_e_matrix(taxa_name,i_m,t_m,e_m, path):\n",
    "    '''\n",
    "    Prints out initial, transition and\n",
    "    emission matrices. Outputs them in \n",
    "    path (parameter) directory as well.\n",
    "    '''\n",
    "    print('initial matrix:')\n",
    "    print()\n",
    "    print(np.exp(i_m))\n",
    "    print()\n",
    "    print('===================================')\n",
    "    print('transition matrix:')\n",
    "    print()\n",
    "    print(np.exp(t_m))\n",
    "    print()\n",
    "    print('===================================')\n",
    "    print('emission matrix:')\n",
    "    print()\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    for i, state in enumerate(np.exp(e_m)):\n",
    "        print('state: ',str(i+1))\n",
    "        print(pd.DataFrame(state,columns=taxa_name,index=['mean','variance']).T)\n",
    "        print()\n",
    "    pd.reset_option('display.max_columns')\n",
    "\n",
    "    np.savetxt(path+'/initial_matrix.txt', np.exp(i_m), delimiter=',')\n",
    "    np.savetxt(path+'/transition_matrix.txt', np.exp(t_m), delimiter=',')\n",
    "    \n",
    "    for i, state in enumerate(np.exp(e_m)):\n",
    "        tmp = pd.DataFrame(state,columns=taxa_name,index=['mean','variance']).T\n",
    "        np.savetxt(path+'/emission_matrix_state_'+str(i+1)+'.txt', tmp.values, delimiter=',')\n",
    "        \n",
    "\n",
    "def output_top_x_variance_taxa_per_state(taxa_names, e_m, n, filepath):\n",
    "    '''\n",
    "    Prints taxa with top n (parameter) variance \n",
    "    per state. Outputs it in filepath (parameter) directory\n",
    "    as txt file as well.\n",
    "    \n",
    "    Input: \n",
    "    1. OTU df\n",
    "    2. emission matrix\n",
    "    3. n (number of taxa to be outputed)\n",
    "    \n",
    "    Example output: \n",
    "    (Bifidobacterium_breve, 10.0)\n",
    "    '''\n",
    "    top_x_variance_taxa_per_state = []\n",
    "    for i in range(len(e_m)):\n",
    "        tmp = []\n",
    "        for j in range(len(taxa_names)):\n",
    "            tmp.append((taxa_names.iloc[j], e_m[i,1,j])) #bind taxa name and variance as tuple\n",
    "        top_x_variance_taxa_per_state.append(tmp)\n",
    "\n",
    "    for i in range(len(top_x_variance_taxa_per_state)):\n",
    "        top_x_variance_taxa_per_state[i] = sorted(top_x_variance_taxa_per_state[i],key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    for i,s in enumerate(top_x_variance_taxa_per_state):\n",
    "        print('state '+str(i+1))\n",
    "        for j,taxon in enumerate(s):\n",
    "            if j < n:\n",
    "                print(taxon)\n",
    "        print()\n",
    "\n",
    "    with open(filepath + '/top_x_variance_taxa_per_state.txt','w') as f:\n",
    "        for i,s in enumerate(top_x_variance_taxa_per_state):\n",
    "            f.write('state '+str(i+1))\n",
    "            for j,taxon in enumerate(s):\n",
    "                if j < n:\n",
    "                    f.write(str(taxon))\n",
    "            f.write('\\n')\n",
    "\n",
    "def hmm_output(taxa_names, i_m, t_m, e_m, n_taxa, score):\n",
    "    '''\n",
    "    Outputs initial matrix, transition matrix, \n",
    "    emission matrix and top n_taxa with the \n",
    "    highest variance as txt file. Also outputs change \n",
    "    in likelihood over iterations.\n",
    "    '''\n",
    "    if os.path.exists(\"hmm_outputs\") == False:\n",
    "        os.mkdir('hmm_outputs')\n",
    "\n",
    "    new_path = make_unique_file_or_dir_names('hmm_outputs/hmm_out')\n",
    "    os.mkdir(new_path)\n",
    "    \n",
    "    output_i_t_e_matrix(taxa_names, i_m, t_m, e_m, new_path)\n",
    "    output_top_x_variance_taxa_per_state(taxa_names, e_m, n_taxa, new_path)\n",
    "    output_likelihood_graph(score, new_path)\n",
    "    print('Output saved in:'+new_path)\n",
    "\n",
    "\n",
    "def run_hmm(data, n_states, taxa_names, likelihood_threshold, n_taxa_for_display):\n",
    "    # EM initialization: calculate first initial, transition, emission matrix\n",
    "    init_i_m= generate_initial_matrix(data,n_states)\n",
    "    init_t_m = generate_transition_matrix(data, n_states)\n",
    "    obs_based_on_states = divide_observations_based_on_states(data, n_states)\n",
    "    df_e_m = calc_mean_var_of_each_state(obs_based_on_states, n_states)\n",
    "    init_e_m = e_m_to_np(df_e_m)\n",
    "    print()\n",
    "    print('Initial, transition and emission matrix generated.')\n",
    "    \n",
    "    print()\n",
    "    print('Moving on to EM algorithm...')\n",
    "    likelihoods, i_m, t_m, e_m = em_algo(data, init_i_m, init_t_m, init_e_m, threshold = likelihood_threshold)\n",
    "    \n",
    "    print()\n",
    "    print('EM algorithm outputs: ')\n",
    "    hmm_output(taxa_names, \\\n",
    "                        i_m, \\\n",
    "                        t_m, \\\n",
    "                        e_m, \\\n",
    "                        n_taxa_for_display, \\\n",
    "                        likelihoods)\n",
    "\n",
    "##### Functions for outputting predictions (classification of GN or GF) #####\n",
    "\n",
    "##### read in initial, transsition & emission matrix from hmm out #####\n",
    "\n",
    "def read_in_i_m_from_hmm_output(path_to_hmm_out):\n",
    "    '''\n",
    "    Returns initial matrix read \n",
    "    from hmm_out directory. Output\n",
    "    is in log space.\n",
    "    \n",
    "    *hmm_out directory might be named\n",
    "    differently depending on the number\n",
    "    of times the code was run.\n",
    "    ex) \n",
    "    If HMM was run twice, there could be:\n",
    "    hmm_out, hmm_out_2\n",
    "    '''\n",
    "    i_m_df = pd.read_csv(path_to_hmm_out+'/initial_matrix.txt',header=None)\n",
    "    return np.log(i_m_df).to_numpy()\n",
    "\n",
    "def read_in_t_m_from_hmm_output(path_to_hmm_out):\n",
    "    '''\n",
    "    Returns transition matrix read \n",
    "    from hmm_out directory. Output\n",
    "    is in log space.\n",
    "    \n",
    "    *hmm_out directory might be named\n",
    "    differently depending on the number\n",
    "    of times the hmm model was run.\n",
    "    ex) \n",
    "    If HMM was run twice, there could be:\n",
    "    hmm_out, hmm_out_(1) \n",
    "    '''\n",
    "    t_m_df = pd.read_csv(path_to_hmm_out+'/transition_matrix.txt',header=None)\n",
    "    return np.log(t_m_df).to_numpy()\n",
    "\n",
    "def read_in_e_m_from_hmm_output(path_to_hmm_out, n_states):\n",
    "    '''\n",
    "    Returns the emission matrix \n",
    "    as np.array with values in log space.\n",
    "    \n",
    "    Input:\n",
    "    1.Path_to_hmm_out parameter is \n",
    "    the path to hmm_out directory.\n",
    "    Do not put the full path to\n",
    "    emssion_matrix.txt file.\n",
    "    \n",
    "    2. Number of states of the HMM model\n",
    "    \n",
    "    *hmm_out directory might be named\n",
    "    differently depending on the number\n",
    "    of times the hmm model was run.\n",
    "    ex) \n",
    "    If HMM was run twice, there could be:\n",
    "    hmm_out, hmm_out_(1) \n",
    "    '''\n",
    "    n_taxa = len(pd.read_csv(path_to_hmm_out+'/emission_matrix_state_1.txt',header=None)) #get taxa length from file\n",
    "    e_m = np.zeros((n_states,2,n_taxa))\n",
    "    for i in range(n_states):\n",
    "        em_state = pd.read_csv(path_to_hmm_out+'/emission_matrix_state_'+str(i+1)+'.txt',header=None)\n",
    "        em_state.columns=['mu','var']\n",
    "        e_m[i,0] = np.log(em_state['mu'])\n",
    "        e_m[i,1] = np.log(em_state['var'])\n",
    "    return e_m\n",
    "\n",
    "def output_im_tm_em_from_hmmout_dir(path, n_states =6):\n",
    "    '''\n",
    "    path = path to hmm_out dir\n",
    "    '''\n",
    "    i_m = read_in_i_m_from_hmm_output(path)\n",
    "    t_m = read_in_t_m_from_hmm_output(path)\n",
    "    e_m = read_in_e_m_from_hmm_output(path, n_states)\n",
    "    return i_m, t_m, e_m\n",
    "\n",
    "def convert_otu_to_data_for_prediction(csv_data_filepath, pma_start_date=196):\n",
    "    '''\n",
    "    difference:\n",
    "    data only contains infants with abundance data.\n",
    "    Abundance data contains nan values for missing samples in 10 timepoints.\n",
    "    '''\n",
    "    test_otu_df= read_data_as_df(csv_data_filepath)\n",
    "    test_otu_df = convert_data_to_int(test_otu_df) # columns of test_otu_df sorted\n",
    "    test_otu_df, taxa_names = choose_top_x_percent_of_taxa_with_highest_variance(test_otu_df, frac=1)\n",
    "\n",
    "    test_otu_df = drop_infants_with_less_than_5_samples(test_otu_df)\n",
    "    timepoints = select_10_pma(pma_start_date=pma_start_date) \n",
    "    selected_columns = sample_based_on_timepoints(test_otu_df, timepoints)\n",
    "    # trim columns (samples) 2\n",
    "    test_otu_df = drop_samples_not_part_of_timepoints(selected_columns, test_otu_df)\n",
    "    data = convert_otu_df_to_data(test_otu_df,include_nan=True)\n",
    "    data = drop_infants_with_less_than_5_after_selecting_infants(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def calc_alphas_for_classification(infant, i_m, t_m, e_m, n_states):\n",
    "    '''\n",
    "    Returns data with alpha matrix \n",
    "    appended to each infant in data.\n",
    "    Input:\n",
    "    1. data (list of infants)\n",
    "    2. initial matrix (np.array())\n",
    "    3. transition matrix (np.array())\n",
    "    4. emisison matrix (np.array())\n",
    "    '''\n",
    "    if len(infant)==1:\n",
    "        infant.append(calc_alpha(infant,i_m,t_m,e_m,n_states))\n",
    "    else:\n",
    "        print('Length of each infant is bigger than 1. Each infant should only contain abundance data.')\n",
    "    return infant\n",
    "\n",
    "def calc_p_of_o(infant):\n",
    "    '''\n",
    "    Returns caclulated P(O)s of all infants\n",
    "    in data in a list.\n",
    "    Input: data (list of infants)\n",
    "    '''\n",
    "    n_tps = len(list(infant[0]))\n",
    "    \n",
    "    p_of_o = logsumexp(infant[1][:,n_tps-1])\n",
    "    return p_of_o\n",
    "\n",
    "def output_p_of_o_for_infant(test_infant, np_i_m, np_t_m, np_e_m, n_states): #path_to_test_otu_table\n",
    "    '''\n",
    "    Returns df with infant id and p(O).\n",
    "    P(O) is in natural log space as\n",
    "    the values are very small.\n",
    "    \n",
    "    Input:\n",
    "    im,tm,em in log space\n",
    "    '''\n",
    "    #preprocess test otu_df data\n",
    "    copy_infant = copy.deepcopy(test_infant)\n",
    "    \n",
    "    copy_infant = calc_alphas_for_classification(copy_infant, np_i_m, np_t_m, np_e_m, n_states)\n",
    "    p_of_o = calc_p_of_o(copy_infant)\n",
    "    return p_of_o\n",
    "\n",
    "def output_p_of_o_df(path_to_hmm_out, test_data, n_states):\n",
    "    '''\n",
    "    takes test data\n",
    "    Output df \n",
    "\n",
    "    column1: infant ID\n",
    "    Column2: predicted label (0 or 1)\n",
    "    0 = GN\n",
    "    1 = GF\n",
    "\n",
    "    prediction = P(O) = probability of observed data\n",
    "    '''\n",
    "    prediction_df = pd.DataFrame({'id':[],'prediction':[]})\n",
    "    i_m, t_m, e_m = output_im_tm_em_from_hmmout_dir(path_to_hmm_out)\n",
    "    \n",
    "    for infant in test_data:\n",
    "        infant_id = re.search('[0-9]+',list(infant[0])[0]).group()\n",
    "        p_of_o = output_p_of_o_for_infant(infant, i_m, t_m, e_m, n_states)\n",
    "        prediction_df = pd.concat([prediction_df, pd.Series([infant_id, p_of_o], index=['id','prediction']).to_frame().T])\n",
    "                \n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
